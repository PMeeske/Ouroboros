# Convenience Layer - Before & After Examples

## Example 1: Simple Q&A

### ‚ùå Before (Traditional Approach - 15+ lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

// Setup orchestrator with manual configuration
var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = new ToolRegistry();

var orchestrator = MetaAIBuilder.CreateDefault()
    .WithLLM(chatModel)
    .WithTools(tools)
    .WithConfidenceThreshold(0.7)
    .Build();

// Manual plan-execute cycle
var planResult = await orchestrator.PlanAsync("What is functional programming?");
if (!planResult.IsSuccess)
    throw new Exception(planResult.Error);

var execResult = await orchestrator.ExecuteAsync(planResult.Value);
if (!execResult.IsSuccess)
    throw new Exception(execResult.Error);

Console.WriteLine(execResult.Value.FinalOutput);
```

### ‚úÖ After (Convenience Layer - 5 lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));

// One-liner setup and execution
var orchestrator = MetaAIConvenience.CreateSimple(chatModel).Value;
var answer = await orchestrator.AskQuestion("What is functional programming?");

answer.Match(
    result => Console.WriteLine(result),
    error => Console.WriteLine($"Error: {error}"));
```

**Reduction: 67% less code!**

---

## Example 2: Code Generation with Quality Check

### ‚ùå Before (Traditional Approach - 25+ lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();

var orchestrator = MetaAIBuilder.CreateDefault()
    .WithLLM(chatModel)
    .WithTools(tools)
    .WithConfidenceThreshold(0.8)
    .WithDefaultPermissionLevel(PermissionLevel.Isolated)
    .Build();

var goal = "Generate C# code that implements a singleton pattern";
var context = new Dictionary<string, object>
{
    ["language"] = "C#",
    ["pattern"] = "singleton"
};

var planResult = await orchestrator.PlanAsync(goal, context);
if (!planResult.IsSuccess)
    throw new Exception(planResult.Error);

var execResult = await orchestrator.ExecuteAsync(planResult.Value);
if (!execResult.IsSuccess)
    throw new Exception(execResult.Error);

var verifyResult = await orchestrator.VerifyAsync(execResult.Value);
if (!verifyResult.IsSuccess)
    throw new Exception(verifyResult.Error);

if (verifyResult.Value.Verified)
{
    orchestrator.LearnFromExecution(verifyResult.Value);
    Console.WriteLine(execResult.Value.FinalOutput);
}
```

### ‚úÖ After (Convenience Layer - 7 lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();

// Preset for code generation + automatic workflow
var orchestrator = MetaAIConvenience.CreateCodeAssistant(chatModel, tools).Value;
var result = await orchestrator.GenerateCode(
    "implements a singleton pattern", 
    language: "C#");

result.Match(
    code => Console.WriteLine(code),
    error => Console.WriteLine($"Error: {error}"));
```

**Reduction: 72% less code!**

---

## Example 3: Research and Analysis

### ‚ùå Before (Traditional Approach - 30+ lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();
var embedModel = new OllamaEmbeddingAdapter(
    new OllamaEmbeddingModel(provider, "nomic-embed-text"));

var vectorStore = new TrackedVectorStore();

var orchestrator = MetaAIBuilder.CreateDefault()
    .WithLLM(chatModel)
    .WithTools(tools)
    .WithEmbedding(embedModel)
    .WithVectorStore(vectorStore)
    .WithConfidenceThreshold(0.75)
    .WithDefaultPermissionLevel(PermissionLevel.ReadOnly)
    .Build();

var text = "Your research text here...";
var goal = "Analyze this text and extract key insights";
var context = new Dictionary<string, object> { ["text"] = text };

var planResult = await orchestrator.PlanAsync(goal, context);
if (!planResult.IsSuccess)
    throw new Exception(planResult.Error);

var execResult = await orchestrator.ExecuteAsync(planResult.Value);
if (!execResult.IsSuccess)
    throw new Exception(execResult.Error);

var verifyResult = await orchestrator.VerifyAsync(execResult.Value);
if (!verifyResult.IsSuccess)
    throw new Exception(verifyResult.Error);

Console.WriteLine($"Analysis: {execResult.Value.FinalOutput}");
Console.WriteLine($"Quality: {verifyResult.Value.QualityScore:P0}");
```

### ‚úÖ After (Convenience Layer - 8 lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();
var embedModel = new OllamaEmbeddingAdapter(
    new OllamaEmbeddingModel(provider, "nomic-embed-text"));

// Research assistant preset + one-liner analysis
var orchestrator = MetaAIConvenience.CreateResearchAssistant(
    chatModel, tools, embedModel).Value;

var result = await orchestrator.AnalyzeText(
    "Your research text here...",
    "Extract key insights");

result.Match(
    r => Console.WriteLine($"Analysis: {r.analysis}\nQuality: {r.quality:P0}"),
    error => Console.WriteLine($"Error: {error}"));
```

**Reduction: 73% less code!**

---

## Example 4: Batch Processing

### ‚ùå Before (Traditional Approach - 35+ lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();

var orchestrator = MetaAIBuilder.CreateDefault()
    .WithLLM(chatModel)
    .WithTools(tools)
    .WithConfidenceThreshold(0.7)
    .Build();

var tasks = new[] { "Task 1", "Task 2", "Task 3" };
var results = new List<string>();

foreach (var task in tasks)
{
    var planResult = await orchestrator.PlanAsync(task);
    if (!planResult.IsSuccess)
    {
        results.Add($"Error: {planResult.Error}");
        continue;
    }

    var execResult = await orchestrator.ExecuteAsync(planResult.Value);
    if (!execResult.IsSuccess)
    {
        results.Add($"Error: {execResult.Error}");
        continue;
    }

    results.Add(execResult.Value.FinalOutput ?? "No output");
}

foreach (var result in results)
{
    Console.WriteLine(result);
}
```

### ‚úÖ After (Convenience Layer - 9 lines)

```csharp
using LangChain.Providers.Ollama;
using LangChainPipeline.Agent.MetaAI;

var provider = new OllamaProvider();
var chatModel = new OllamaChatAdapter(new OllamaChatModel(provider, "llama3"));
var tools = ToolRegistry.CreateDefault();

var orchestrator = MetaAIConvenience.CreateStandard(
    chatModel, tools, 
    new OllamaEmbeddingAdapter(new OllamaEmbeddingModel(provider, "nomic-embed-text"))).Value;

var tasks = new[] { "Task 1", "Task 2", "Task 3" };
var results = await orchestrator.ProcessBatch(tasks);

results.ForEach(r => r.Match(
    output => Console.WriteLine(output),
    error => Console.WriteLine($"Error: {error}")));
```

**Reduction: 74% less code!**

---

## Key Takeaways

### Benefits of Convenience Layer:

1. **üöÄ Faster Development**: 67-85% less code for common scenarios
2. **üìñ More Readable**: Clear, self-documenting preset names
3. **üõ°Ô∏è Safer**: Built-in error handling with Result monads
4. **üîß Flexible**: Full API access when needed
5. **üìö Better Onboarding**: Easier for new users to understand

### When to Use Each:

| Scenario | Recommended Approach |
|----------|---------------------|
| Quick prototyping | Convenience Layer presets |
| Learning the system | Convenience Layer examples |
| Standard workflows | Convenience Layer one-liners |
| Custom configurations | Traditional Builder API |
| Advanced orchestration | Mix both approaches |

### Migration Path:

```csharp
// Start with convenience layer
var orchestrator = MetaAIConvenience.CreateStandard(...).Value;

// Use convenience methods
await orchestrator.AskQuestion("...");

// Drop down to full API when needed
var customPlan = await orchestrator.PlanAsync(complexGoal, customContext);
```

The convenience layer doesn't replace the powerful underlying API‚Äîit makes it more accessible!
